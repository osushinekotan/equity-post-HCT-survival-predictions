{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import config\n",
    "import lightning as L\n",
    "import polars as pl\n",
    "from preprocess import fe, load_data, preprocess\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from src.customs.fold import add_kfold\n",
    "from src.customs.metrics import metric\n",
    "from src.trainer.tabular.simple import single_inference_fn, single_train_fn\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = load_data(config=config, valid_ratio=config.VALID_RATIO)\n",
    "target_df = pl.read_csv(\"./data/extr_output/101/1/101.csv\").with_columns(\n",
    "    pl.col(config.SURVIVAL_TIME_COL).log().alias(\"t_log_efs_time\"),\n",
    ")\n",
    "target_cols = [x for x in target_df.columns if x.startswith(\"t_\")]\n",
    "train_test_df = train_test_df.join(\n",
    "    target_df.select(\n",
    "        [\n",
    "            config.ID_COL,\n",
    "            *target_cols,\n",
    "        ],\n",
    "    ),\n",
    "    on=config.ID_COL,\n",
    "    how=\"left\",\n",
    ")\n",
    "config.META_COLS = set(config.META_COLS) | set(target_cols)\n",
    "\n",
    "features_df = fe(config=config, train_test_df=train_test_df)\n",
    "features_df = preprocess(config=config, features_df=features_df)\n",
    "feature_names = sorted([x for x in features_df.columns if x.startswith(config.FEATURE_PREFIX)])\n",
    "cat_features = [x for x in feature_names if x.startswith(f\"{config.FEATURE_PREFIX}c_\")]\n",
    "\n",
    "\n",
    "def make_new_targets(\n",
    "    df: pl.DataFrame,\n",
    "    base_target_names: tuple[str] = (\"t_kmf\", \"t_bfhf\"),\n",
    "    lower_bound_pos: float = 0.0,\n",
    "    lower_bound_neg: float = 0.0,\n",
    "    pred_col: str = \"t_event_pred\",\n",
    ") -> pl.DataFrame:\n",
    "    for base_target_name in base_target_names:\n",
    "        new_target_name = f\"{base_target_name}_event_scaled2\"\n",
    "\n",
    "        scaling_factor_pos = (\n",
    "            df.filter(pl.col(config.EVENT_COL) == 1)\n",
    "            .select(pl.col(pred_col).log().min() / (lower_bound_pos - pl.col(base_target_name).min()))[pred_col]\n",
    "            .to_numpy()[0]\n",
    "        )\n",
    "        scaling_factor_neg = (\n",
    "            df.filter(pl.col(config.EVENT_COL) == 0)\n",
    "            .select(pl.col(pred_col).log().min() / (lower_bound_neg - pl.col(base_target_name).min()))[pred_col]\n",
    "            .to_numpy()[0]\n",
    "        )\n",
    "\n",
    "        print(scaling_factor_pos, scaling_factor_neg)\n",
    "\n",
    "        new_df = df.select(\n",
    "            pl.col(config.ID_COL),\n",
    "            pl.when(pl.col(config.EVENT_COL) == 1)\n",
    "            .then(pl.col(pred_col).log() / scaling_factor_pos + pl.col(base_target_name))\n",
    "            .otherwise(pl.col(pred_col).log() / scaling_factor_neg + pl.col(base_target_name))\n",
    "            .alias(new_target_name),\n",
    "        )\n",
    "        df = df.join(new_df, on=config.ID_COL, how=\"left\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "features_df = features_df.join(\n",
    "    make_new_targets(\n",
    "        target_df,\n",
    "        lower_bound_neg=0.0,\n",
    "        lower_bound_pos=0.0,\n",
    "    ).select(pl.col(config.ID_COL), pl.col(\"t_kmf_event_scaled2\")),\n",
    "    on=config.ID_COL,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "max_val = features_df.filter(pl.col(config.EVENT_COL) == 0)[config.SURVIVAL_TIME_COL].max()\n",
    "min_val = features_df.filter(pl.col(config.EVENT_COL) == 0)[config.SURVIVAL_TIME_COL].min()\n",
    "\n",
    "features_df = features_df.with_columns(\n",
    "    # min-max scaling\n",
    "    (1 - ((pl.col(config.SURVIVAL_TIME_COL) - min_val) / (max_val - min_val))).alias(\"scaled_survival_time\")\n",
    ").with_columns(\n",
    "    pl.when(pl.col(config.EVENT_COL) == 0)\n",
    "    .then(pl.col(\"scaled_survival_time\") * (0.5 - 0.1) + 0.1)\n",
    "    .otherwise(1)\n",
    "    .alias(\"weight\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_tabular.models.common.layers import ODST\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CatEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module for the categorical dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, projection_dim: int, categorical_cardinality: list[int], embedding_dim: int):\n",
    "        \"\"\"\n",
    "        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n",
    "        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n",
    "        embedding_dim: The size of the embedding space for each categorical feature.\n",
    "        self.embeddings: list of embedding layers for each categorical feature.\n",
    "        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(cardinality, embedding_dim) for cardinality in categorical_cardinality]\n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(projection_dim, projection_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat):\n",
    "        \"\"\"\n",
    "        Apply the projection on concatened embeddings that contains all categorical features.\n",
    "        \"\"\"\n",
    "        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        x_cat = torch.cat(x_cat, dim=1)\n",
    "        return self.projection(x_cat)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Train a model on both categorical embeddings and numerical data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        continuous_dim: int,\n",
    "        categorical_cardinality: list[int],\n",
    "        embedding_dim: int,\n",
    "        projection_dim: int,\n",
    "        hidden_dim: int,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = CatEmbedding(projection_dim, categorical_cardinality, embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            ODST(projection_dim + continuous_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.main_out = nn.Linear(hidden_dim, 1)\n",
    "        self.aux_out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Create embedding layers for categorical data, concatenate with continous variables.\n",
    "        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x_cat)\n",
    "        x = torch.cat([x, x_cont], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mlp(x)\n",
    "        return self.main_out(x), self.aux_out(x)\n",
    "\n",
    "\n",
    "@functools.lru_cache\n",
    "def combinations(N):\n",
    "    \"\"\"\n",
    "    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1,\n",
    "    and caches the result using functools.lru_cache for optimization\n",
    "    \"\"\"\n",
    "    ind = torch.arange(N)\n",
    "    comb = torch.combinations(ind, r=2)\n",
    "    return comb.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Main Model creation and losses definition to fully train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,  # partial\n",
    "        scheduler: torch.optim.lr_scheduler,  # partial\n",
    "        aux_weight: float = 0.1,\n",
    "        margin: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)  # to use self.hparams\n",
    "\n",
    "        # Creates an instance of the NN model defined above\n",
    "        self.model = net\n",
    "        self.results = []\n",
    "\n",
    "        # for averaging loss across batches\n",
    "        self.train_main_loss = MeanMetric()\n",
    "        self.train_loss = MeanMetric()\n",
    "\n",
    "        self.val_main_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, x_aux = self.model(x_cat=batch[\"x_cat\"], x_cont=batch[\"x_cont\"])\n",
    "        return x.squeeze(1), x_aux.squeeze(1)\n",
    "\n",
    "    def model_step(self, batch):\n",
    "        y_pred, aux_pred = self.forward(batch)\n",
    "        main_loss = self.calc_main_loss(time=batch[\"time\"], event=batch[\"event\"], y_pred=y_pred)\n",
    "        aux_loss = nn.functional.mse_loss(aux_pred, batch[\"aux_target\"], reduction=\"mean\")\n",
    "        loss = main_loss + (aux_loss * self.hparams.aux_weight)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"main_loss\": main_loss,\n",
    "            \"aux_loss\": aux_loss,\n",
    "            \"y_pred\": y_pred,\n",
    "            \"aux_pred\": aux_pred,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Perform a single training step on a batch of data from the training set.\n",
    "\n",
    "        :param batch: A batch of data (a tuple) containing the input tensor of images and target\n",
    "            labels.\n",
    "        :param batch_idx: The index of the current batch.\n",
    "        :return: A tensor of losses between model predictions and targets.\n",
    "        \"\"\"\n",
    "        result = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.train_main_loss(result[\"main_loss\"])\n",
    "        self.train_loss(result[\"loss\"])\n",
    "        self.log(\"train/main_loss\", self.train_main_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return result[\"loss\"]\n",
    "\n",
    "    def calc_main_loss(self, time, event, y_pred):\n",
    "        N = time.shape[0]\n",
    "        pairs = combinations(N)\n",
    "\n",
    "        # 比較可能な候補ペアのみを取得\n",
    "        pairs = pairs[(event[pairs[:, 0]] == 1) | (event[pairs[:, 1]] == 1)]\n",
    "\n",
    "        left_index, right_index = pairs[:, 0], pairs[:, 1]\n",
    "        time_left, time_right = time[left_index], time[right_index]\n",
    "        event_left, event_right = event[left_index], event[right_index]\n",
    "        y_pred_left, y_pred_right = y_pred[left_index], y_pred[right_index]\n",
    "\n",
    "        # calculate the loss\n",
    "        y = 2 * (time_left > time_right).int() - 1\n",
    "        loss = nn.functional.relu(-y * (y_pred_right - y_pred_left) + self.hparams.margin)\n",
    "\n",
    "        # loss 計算対象 mask\n",
    "        mask = self._get_mask(\n",
    "            time_left=time_left,\n",
    "            time_right=time_right,\n",
    "            event_left=event_left,\n",
    "            event_right=event_right,\n",
    "        )\n",
    "        loss = (loss.double() * (mask.double())).sum() / mask.sum()  # mean across batch\n",
    "        return loss\n",
    "\n",
    "    def _get_mask(self, time_left, time_right, event_left, event_right):\n",
    "        # Case 1: left が right より生存時間が長いが、right が censored されている場合\n",
    "        left_outlived = time_left >= time_right\n",
    "        left_1_right_0 = (event_left) & (event_right == 0)\n",
    "        mask = left_outlived & left_1_right_0\n",
    "\n",
    "        # Case 2: right が left より生存時間が長いが、left が censored されている場合\n",
    "        right_outlived = time_right >= time_left\n",
    "        right_1_left_0 = (event_right == 1) & (event_left == 0)\n",
    "\n",
    "        # Combine the masks\n",
    "        mask |= right_outlived & right_1_left_0\n",
    "        return ~mask  # Invert the mask to get the valid pairs\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        result = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.val_main_loss(result[\"main_loss\"])\n",
    "        self.val_loss(result[\"loss\"])\n",
    "        self.log(\"val/main_loss\", self.val_main_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # store time, event, y_pred, aux_pred for c-index calculation\n",
    "        self.results.append(\n",
    "            {\n",
    "                **{k: v for k, v in result.items() if k in [\"y_pred\", \"aux_pred\"]},\n",
    "                **{k: v for k, v in batch.items() if k in [\"time\", \"event\", \"race_group\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def calc_metric(\n",
    "        self,\n",
    "        time: np.ndarray,\n",
    "        event: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        aux_pred: np.ndarray,\n",
    "        blend_pred: np.ndarray,\n",
    "        race_group: np.ndarray,\n",
    "    ):\n",
    "        score_y = metric(y_time=time, y_event=event, y_pred=y_pred, race_group=race_group)\n",
    "        score_aux = metric(y_time=time, y_event=event, y_pred=aux_pred, race_group=race_group)\n",
    "        score_blend = metric(y_time=time, y_event=event, y_pred=blend_pred, race_group=race_group)\n",
    "        return score_y, score_aux, score_blend\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        At the end of the validation epoch, it computes and logs the concordance index\n",
    "        \"\"\"\n",
    "        time = np.concatenate([r[\"time\"].cpu().numpy() for r in self.results])\n",
    "        event = np.concatenate([r[\"event\"].cpu().numpy() for r in self.results])\n",
    "        y_pred = np.concatenate([r[\"y_pred\"].cpu().numpy() for r in self.results])\n",
    "        aux_pred = np.concatenate([r[\"aux_pred\"].cpu().numpy() for r in self.results])\n",
    "        race_group = np.concatenate([r[\"race_group\"].cpu().numpy() for r in self.results])\n",
    "\n",
    "        # ensemble: rankdata -> normalize -> mean\n",
    "        blend_pred = np.mean([np.argsort(y_pred) / len(y_pred), np.argsort(aux_pred) / len(aux_pred)], axis=0)\n",
    "\n",
    "        score_y, score_aux, score_blend = self.calc_metric(\n",
    "            time=time,\n",
    "            event=event,\n",
    "            y_pred=y_pred,\n",
    "            aux_pred=aux_pred,\n",
    "            blend_pred=blend_pred,\n",
    "            race_group=race_group,\n",
    "        )\n",
    "        self.log(\"val/score\", score_y, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/aux_score\", score_aux, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/blend_score\", score_blend, on_epoch=True, prog_bar=False, logger=True)\n",
    "\n",
    "        self.results = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())\n",
    "        if self.hparams.scheduler is not None:\n",
    "            scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val/loss\",\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        return {\"optimizer\": optimizer}\n",
    "\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        cat_feature_cols: list[str],\n",
    "        cont_feature_cols: list[str],\n",
    "        aux_target_col: str,\n",
    "        time_col: str,\n",
    "        event_col: str,\n",
    "        race_group_col: str,\n",
    "    ):\n",
    "        self.cat_features = df.select(cat_feature_cols).to_numpy()\n",
    "        self.cont_features = df.select(cont_feature_cols).to_numpy()\n",
    "        self.time = df[time_col].to_numpy()\n",
    "        self.event = df[event_col].to_numpy()\n",
    "        self.aux_target = df[aux_target_col].to_numpy()\n",
    "        self.race_group = df[race_group_col].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"x_cat\": torch.tensor(self.cat_features[idx], dtype=torch.long),\n",
    "            \"x_cont\": torch.tensor(self.cont_features[idx], dtype=torch.float),\n",
    "            \"time\": torch.tensor(self.time[idx], dtype=torch.float),\n",
    "            \"event\": torch.tensor(self.event[idx], dtype=torch.long),\n",
    "            \"aux_target\": torch.tensor(self.aux_target[idx], dtype=torch.float),\n",
    "            \"race_group\": torch.tensor(self.race_group[idx]),\n",
    "        }\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        cat_feature_cols: list[str],\n",
    "        cont_feature_cols: list[str],\n",
    "        race_group_col: str,\n",
    "    ):\n",
    "        self.cat_features = df.select(cat_feature_cols).to_numpy()\n",
    "        self.cont_features = df.select(cont_feature_cols).to_numpy()\n",
    "        self.race_group = df[race_group_col].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"x_cat\": torch.tensor(self.cat_features[idx], dtype=torch.long),\n",
    "            \"x_cont\": torch.tensor(self.cont_features[idx], dtype=torch.float),\n",
    "            \"race_group\": torch.tensor(self.race_group[idx]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "va_result_df, va_scores = pl.DataFrame(), {}\n",
    "\n",
    "for seed in config.SEEDS:\n",
    "    L.seed_everything(seed)\n",
    "    name = f\"pair_{seed}\"\n",
    "    i_features_df = add_kfold(\n",
    "        features_df,\n",
    "        n_splits=config.N_SPLITS,\n",
    "        random_state=seed,\n",
    "        fold_col=config.FOLD_COL,\n",
    "    )\n",
    "    for i_fold in i_features_df[config.FOLD_COL].unique().sort():\n",
    "        fold_name = f\"fold_{i_fold:02}\"\n",
    "        tr_dataloader = torch.utils.data.DataLoader(\n",
    "            TrainDataset(\n",
    "                df=i_features_df.filter(pl.col(config.FOLD_COL) != i_fold),\n",
    "                cat_feature_cols=cat_features,\n",
    "                cont_feature_cols=feature_names,\n",
    "                aux_target_col=\"t_kmf_event_scaled2\",\n",
    "                time_col=config.SURVIVAL_TIME_COL,\n",
    "                event_col=config.EVENT_COL,\n",
    "                race_group_col=\"f_c_race_group\",\n",
    "            ),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        va_dataloader = torch.utils.data.DataLoader(\n",
    "            TrainDataset(\n",
    "                df=i_features_df.filter(pl.col(config.FOLD_COL) == i_fold),\n",
    "                cat_feature_cols=cat_features,\n",
    "                cont_feature_cols=feature_names,\n",
    "                aux_target_col=\"t_kmf_event_scaled2\",\n",
    "                time_col=config.SURVIVAL_TIME_COL,\n",
    "                event_col=config.EVENT_COL,\n",
    "                race_group_col=\"f_c_race_group\",\n",
    "            ),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        # Create the model\n",
    "        net = Net(\n",
    "            continuous_dim=len(feature_names),\n",
    "            categorical_cardinality=[features_df[f].n_unique() + 1 for f in cat_features],  # full cardinality\n",
    "            embedding_dim=16,\n",
    "            projection_dim=24,\n",
    "            hidden_dim=64,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        optimizer = functools.partial(torch.optim.AdamW, lr=5e-4, weight_decay=1e-5)\n",
    "        scheduler = functools.partial(torch.optim.lr_scheduler.CosineAnnealingLR, T_max=64, eta_min=1e-7)\n",
    "\n",
    "        lit_module = LitModule(\n",
    "            net=net,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            aux_weight=0.1,\n",
    "            margin=0.2,\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            # accelerator=\"cpu\",\n",
    "            max_epochs=100,\n",
    "            callbacks=[\n",
    "                L.pytorch.callbacks.ModelCheckpoint(\n",
    "                    monitor=\"val/loss\",\n",
    "                    dirpath=(config.OUTPUT_DIR / name / fold_name).as_posix(),\n",
    "                    save_top_k=1,\n",
    "                    mode=\"min\",\n",
    "                    enable_version_counter=False,\n",
    "                    auto_insert_metric_name=False,\n",
    "                    filename=\"model\",\n",
    "                    save_weights_only=True,\n",
    "                ),\n",
    "                L.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "                L.pytorch.callbacks.TQDMProgressBar(),\n",
    "                # L.pytorch.callbacks.StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=16, annealing_epochs=16),\n",
    "            ],\n",
    "            logger=L.pytorch.loggers.CSVLogger(\n",
    "                (config.OUTPUT_DIR / name / fold_name).as_posix(), name=\"logs\", version=\"latest\"\n",
    "            ),\n",
    "        )\n",
    "        trainer.fit(lit_module, tr_dataloader, va_dataloader)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
