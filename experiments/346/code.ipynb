{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import config\n",
    "import lightgbm as lgb\n",
    "import polars as pl\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from preprocess import fe, load_data\n",
    "\n",
    "from src.customs.fold import add_kfold\n",
    "from src.customs.metrics import CatBoostMetric, Metric, ROCAUCMetric\n",
    "from src.model.sklearn_like import CatBoostClassifierWrapper, CatBoostRegressorWrapper, LightGBMWapper\n",
    "from src.trainer.tabular.simple import single_inference_fn, single_train_fn\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = load_data(config=config, valid_ratio=config.VALID_RATIO)\n",
    "target_df = pl.read_csv(\"./data/extr_output/101/1/101.csv\").with_columns(pl.col(\"t_event_pred\") * 2)\n",
    "target_cols = [x for x in target_df.columns if x.startswith(\"t_\")]\n",
    "train_test_df = train_test_df.join(\n",
    "    target_df.select(\n",
    "        [\n",
    "            config.ID_COL,\n",
    "            *target_cols,\n",
    "        ],\n",
    "    ),\n",
    "    on=config.ID_COL,\n",
    "    how=\"left\",\n",
    ")\n",
    "config.META_COLS = set(config.META_COLS) | set(target_cols)\n",
    "\n",
    "features_df = fe(config=config, train_test_df=train_test_df)\n",
    "feature_names = sorted([x for x in features_df.columns if x.startswith(config.FEATURE_PREFIX)])\n",
    "cat_features = [x for x in feature_names if x.startswith(f\"{config.FEATURE_PREFIX}c_\")]\n",
    "\n",
    "\n",
    "all_val_result_df = pl.DataFrame()\n",
    "for i, seed in enumerate([0, 1, 2, 3, 4]):\n",
    "    print(f\"start lgb1 {seed}\")\n",
    "    _va_result_df, _, _ = single_train_fn(\n",
    "        model=LightGBMWapper(\n",
    "            name=f\"lgb1_{seed}\",\n",
    "            model=lgb.LGBMModel(\n",
    "                objective=\"binary\",\n",
    "                boosting=\"gbdt\",\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.005,\n",
    "                num_leaves=11 + (i * 5),\n",
    "                colsample_bytree=0.2,\n",
    "                subsample=0.5,\n",
    "                importance_type=\"gain\",\n",
    "                random_state=seed,\n",
    "            ),\n",
    "            fit_params={\n",
    "                \"callbacks\": [\n",
    "                    lgb.early_stopping(300, first_metric_only=True),\n",
    "                    lgb.log_evaluation(period=100),\n",
    "                ],\n",
    "                \"categorical_feature\": cat_features,\n",
    "                \"feature_name\": feature_names,\n",
    "            },\n",
    "        ),\n",
    "        features_df=add_kfold(\n",
    "            features_df,\n",
    "            n_splits=10,\n",
    "            random_state=seed,\n",
    "            fold_col=config.FOLD_COL,\n",
    "        ),\n",
    "        feature_cols=feature_names,\n",
    "        target_col=config.EVENT_COL,\n",
    "        fold_col=config.FOLD_COL,\n",
    "        meta_cols=config.META_COLS,\n",
    "        out_dir=config.OUTPUT_DIR,\n",
    "        train_folds=None,\n",
    "        eval_fn=ROCAUCMetric(),\n",
    "        overwrite=False,\n",
    "        use_eval_metric_extra_va_df=False,\n",
    "    )\n",
    "\n",
    "    all_val_result_df = pl.concat(\n",
    "        [\n",
    "            all_val_result_df,\n",
    "            _va_result_df.select(\n",
    "                [\n",
    "                    pl.col(config.ID_COL),\n",
    "                    pl.col(\"pred\"),\n",
    "                    pl.col(\"name\"),\n",
    "                    pl.col(config.FOLD_COL),\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        how=\"diagonal_relaxed\",\n",
    "    )\n",
    "\n",
    "    print(f\"start catboost1 {seed}\")\n",
    "    _va_result_df, _, _ = single_train_fn(\n",
    "        model=CatBoostClassifierWrapper(\n",
    "            name=f\"cat1_{seed}\",\n",
    "            model=CatBoostClassifier(\n",
    "                loss_function=\"Logloss\",\n",
    "                learning_rate=0.005 + (i * 0.005),\n",
    "                n_estimators=20000,\n",
    "                early_stopping_rounds=300,\n",
    "                verbose=100,\n",
    "                # subsample=0.5,\n",
    "                colsample_bylevel=0.2,\n",
    "                random_state=seed,\n",
    "            ),\n",
    "            multi_output=False,\n",
    "            feature_names=feature_names,\n",
    "            cat_features=cat_features,\n",
    "        ),\n",
    "        features_df=add_kfold(\n",
    "            features_df,\n",
    "            n_splits=config.N_SPLITS,\n",
    "            random_state=seed,\n",
    "            fold_col=config.FOLD_COL,\n",
    "        ),\n",
    "        feature_cols=feature_names,\n",
    "        target_col=config.EVENT_COL,\n",
    "        fold_col=config.FOLD_COL,\n",
    "        meta_cols=config.META_COLS,\n",
    "        out_dir=config.OUTPUT_DIR,\n",
    "        train_folds=None,\n",
    "        eval_fn=ROCAUCMetric(),\n",
    "        overwrite=False,\n",
    "        use_eval_metric_extra_va_df=False,\n",
    "    )\n",
    "    all_val_result_df = pl.concat(\n",
    "        [\n",
    "            all_val_result_df,\n",
    "            _va_result_df.select(\n",
    "                [\n",
    "                    pl.col(config.ID_COL),\n",
    "                    pl.col(\"pred\"),\n",
    "                    pl.col(\"name\"),\n",
    "                    pl.col(config.FOLD_COL),\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        how=\"diagonal_relaxed\",\n",
    "    )\n",
    "\n",
    "# mean ensemble\n",
    "agg_va_result_df = (\n",
    "    all_val_result_df.group_by([config.ID_COL])\n",
    "    .agg(pl.col(\"pred\").mean().alias(\"pred\"))\n",
    "    .join(features_df.select(config.META_COLS), on=config.ID_COL, how=\"left\")\n",
    ")\n",
    "logger.info(f\"{ROCAUCMetric()._name}: {ROCAUCMetric()(agg_va_result_df)}\")\n",
    "\n",
    "\n",
    "def make_new_targets(\n",
    "    df: pl.DataFrame,\n",
    "    base_target_names: tuple[str] = (\"t_kmf\", \"t_bfhf\"),\n",
    "    lower_bound: float = 0.0,\n",
    ") -> pl.DataFrame:\n",
    "    lower_bound = 0.0\n",
    "\n",
    "    for base_target_name in base_target_names:\n",
    "        new_target_name = f\"{base_target_name}_event_scaled\"\n",
    "\n",
    "        scaling_factor = df.select(pl.col(\"pred\").log().min() / (lower_bound - pl.col(base_target_name).min()))[\n",
    "            \"pred\"\n",
    "        ].to_numpy()[0]\n",
    "        print(scaling_factor)\n",
    "\n",
    "        new_df = df.select(\n",
    "            pl.col(config.ID_COL),\n",
    "            (pl.col(\"pred\").log() / scaling_factor + pl.col(base_target_name)).alias(new_target_name),\n",
    "        )\n",
    "        df = df.join(new_df, on=config.ID_COL, how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "target_df = make_new_targets(\n",
    "    df=agg_va_result_df,\n",
    "    base_target_names=(\"t_kmf\", \"t_bfhf\"),\n",
    ")\n",
    "\n",
    "target_cols = [x for x in target_df.columns if x.startswith(\"t_\")]\n",
    "meta_cols = [x for x in config.META_COLS if x not in target_cols]\n",
    "\n",
    "\n",
    "target_df = target_df.select(\n",
    "    [\n",
    "        pl.col(meta_cols),\n",
    "        pl.col(\"pred\").alias(\"t_event_pred\"),\n",
    "        *[pl.col(x) for x in target_cols if x != \"t_event_pred\"],\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = load_data(config=config, valid_ratio=config.VALID_RATIO)\n",
    "target_df = target_df.with_columns(pl.col(\"t_event_pred\") * 2)\n",
    "target_cols = [x for x in target_df.columns if x.startswith(\"t_\")]\n",
    "train_test_df = train_test_df.join(\n",
    "    target_df.select(\n",
    "        [\n",
    "            config.ID_COL,\n",
    "            *target_cols,\n",
    "        ],\n",
    "    ),\n",
    "    on=config.ID_COL,\n",
    "    how=\"left\",\n",
    ")\n",
    "config.META_COLS = set(config.META_COLS) | set(target_cols)\n",
    "\n",
    "features_df = fe(config=config, train_test_df=train_test_df)\n",
    "feature_names = sorted([x for x in features_df.columns if x.startswith(config.FEATURE_PREFIX)])\n",
    "cat_features = [x for x in feature_names if x.startswith(f\"{config.FEATURE_PREFIX}c_\")]\n",
    "\n",
    "\n",
    "def make_new_targets(\n",
    "    df: pl.DataFrame,\n",
    "    base_target_names: tuple[str] = (\"t_kmf\", \"t_bfhf\"),\n",
    "    lower_bound_pos: float = 0.0,\n",
    "    lower_bound_neg: float = 0.0,\n",
    "    pred_col: str = \"t_event_pred\",\n",
    ") -> pl.DataFrame:\n",
    "    for base_target_name in base_target_names:\n",
    "        new_target_name = f\"{base_target_name}_event_scaled2\"\n",
    "\n",
    "        scaling_factor_pos = (\n",
    "            df.filter(pl.col(config.EVENT_COL) == 1)\n",
    "            .select(pl.col(pred_col).log().min() / (lower_bound_pos - pl.col(base_target_name).min()))[pred_col]\n",
    "            .to_numpy()[0]\n",
    "        )\n",
    "        scaling_factor_neg = (\n",
    "            df.filter(pl.col(config.EVENT_COL) == 0)\n",
    "            .select(pl.col(pred_col).log().min() / (lower_bound_neg - pl.col(base_target_name).min()))[pred_col]\n",
    "            .to_numpy()[0]\n",
    "        )\n",
    "\n",
    "        print(scaling_factor_pos, scaling_factor_neg)\n",
    "\n",
    "        new_df = df.select(\n",
    "            pl.col(config.ID_COL),\n",
    "            pl.when(pl.col(config.EVENT_COL) == 1)\n",
    "            .then(pl.col(pred_col).log() / scaling_factor_pos + pl.col(base_target_name))\n",
    "            .otherwise(pl.col(pred_col).log() / scaling_factor_neg + pl.col(base_target_name))\n",
    "            .alias(new_target_name),\n",
    "        )\n",
    "        df = df.join(new_df, on=config.ID_COL, how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "features_df = features_df.join(\n",
    "    make_new_targets(\n",
    "        target_df,\n",
    "        lower_bound_neg=0.0,\n",
    "        lower_bound_pos=0.0,\n",
    "    ).select(pl.col(config.ID_COL), pl.col(\"t_kmf_event_scaled2\").exp()),\n",
    "    on=config.ID_COL,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "\n",
    "def add_weight(\n",
    "    features_df: pl.DataFrame,\n",
    "    survival_time_col: str,\n",
    "    event_col: str,\n",
    "    neg_min: float = 0.01,  # event == 0 の場合の下限\n",
    "    neg_max: float = 0.5,  # event == 0 の場合の上限\n",
    "    pos_min: float = 1.0,  # event == 1 の場合の下限\n",
    "    pos_max: float = 1.5,  # event == 1 の場合の上限\n",
    ") -> pl.DataFrame:\n",
    "    features_df = features_df.with_columns(pl.col(survival_time_col).alias(\"tmp_time\"))\n",
    "\n",
    "    # 各 event グループごとに tmp_time の最小値と最大値を計算\n",
    "    features_df = features_df.with_columns(\n",
    "        [\n",
    "            pl.col(\"tmp_time\").min().over(event_col).alias(\"group_min\"),\n",
    "            pl.col(\"tmp_time\").max().over(event_col).alias(\"group_max\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 各グループごとにスケーリングした値から weight を計算\n",
    "    features_df = features_df.with_columns(\n",
    "        pl.when(pl.col(event_col) == 0)\n",
    "        .then(\n",
    "            # event == 0 の場合は逆スケーリング: 1 - ((tmp_time - group_min) / (group_max - group_min))\n",
    "            (1 - ((pl.col(\"tmp_time\") - pl.col(\"group_min\")) / (pl.col(\"group_max\") - pl.col(\"group_min\"))))\n",
    "            * (neg_max - neg_min)\n",
    "            + neg_min\n",
    "        )\n",
    "        .otherwise(\n",
    "            # event == 1 の場合はそのままスケーリング: ((tmp_time - group_min) / (group_max - group_min))\n",
    "            ((pl.col(\"tmp_time\") - pl.col(\"group_min\")) / (pl.col(\"group_max\") - pl.col(\"group_min\")))\n",
    "            * (pos_max - pos_min)\n",
    "            + pos_min\n",
    "        )\n",
    "        .alias(\"weight\")\n",
    "    )\n",
    "\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_result_df, va_scores = pl.DataFrame(), {}\n",
    "for i, seed in enumerate(config.SEEDS):\n",
    "    name = f\"cat_{seed}\"\n",
    "    _va_result_df, _va_scores, trained_models = single_train_fn(\n",
    "        model=CatBoostRegressorWrapper(\n",
    "            name=name,\n",
    "            model=CatBoostRegressor(\n",
    "                loss_function=\"Tweedie:variance_power=1.5\",\n",
    "                grow_policy=\"SymmetricTree\",\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=100000,\n",
    "                early_stopping_rounds=3000,\n",
    "                eval_metric=CatBoostMetric(),\n",
    "                verbose=100,\n",
    "                random_state=seed,\n",
    "                # subsample=0.9,\n",
    "                colsample_bylevel=0.2,\n",
    "            ),\n",
    "            multi_output=False,\n",
    "            feature_names=feature_names,\n",
    "            cat_features=cat_features,\n",
    "        ),\n",
    "        features_df=add_kfold(\n",
    "            add_weight(\n",
    "                features_df,\n",
    "                survival_time_col=config.SURVIVAL_TIME_COL,\n",
    "                event_col=config.EVENT_COL,\n",
    "                neg_min=0.01,\n",
    "                neg_max=0.5,\n",
    "                pos_min=1.0,\n",
    "                pos_max=1.5,\n",
    "            ),\n",
    "            n_splits=config.N_SPLITS,\n",
    "            random_state=seed,\n",
    "            fold_col=config.FOLD_COL,\n",
    "        ),\n",
    "        feature_cols=feature_names,\n",
    "        target_col=\"t_kmf_event_scaled2\",\n",
    "        fold_col=config.FOLD_COL,\n",
    "        meta_cols=config.META_COLS,\n",
    "        weight_col=\"weight\",\n",
    "        out_dir=config.OUTPUT_DIR,\n",
    "        train_folds=None,\n",
    "        eval_fn=Metric(),\n",
    "        overwrite=False,\n",
    "        use_eval_metric_extra_va_df=True,\n",
    "    )\n",
    "    va_result_df = pl.concat([va_result_df, _va_result_df], how=\"diagonal_relaxed\")\n",
    "    va_scores[name] = _va_scores\n",
    "\n",
    "# ------------------------------\n",
    "# final score\n",
    "# ------------------------------\n",
    "va_result_agg_df = (\n",
    "    va_result_df.group_by(config.ID_COL)\n",
    "    .agg(pl.col(\"pred\").mean())\n",
    "    .sort(\"ID\")\n",
    "    .join(train_test_df.select(config.META_COLS), on=config.ID_COL, how=\"left\")\n",
    ")\n",
    "final_score = Metric()(input_df=va_result_agg_df)\n",
    "logger.info(f\"✅ final score: {final_score}\")\n",
    "va_scores[\"final\"] = final_score\n",
    "\n",
    "# save\n",
    "va_result_agg_df.write_csv(f\"{config.OUTPUT_DIR}/va_result.csv\")\n",
    "with open(f\"{config.OUTPUT_DIR}/va_scores.json\", \"w\") as f:\n",
    "    json.dump(va_scores, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming va_result_agg_df is already defined in your environment\n",
    "# If not, you would need to load it first\n",
    "\n",
    "\n",
    "def calculate_score(threshold, adjustment, input_df=va_result_agg_df):\n",
    "    \"\"\"\n",
    "    Calculate the score with given threshold and adjustment parameters\n",
    "\n",
    "    Args:\n",
    "        threshold: The threshold value to replace 0.7\n",
    "        adjustment: The adjustment value to replace 0.1\n",
    "        input_df: The input dataframe\n",
    "\n",
    "    Returns:\n",
    "        score: The calculated metric score\n",
    "    \"\"\"\n",
    "    adjusted_df = input_df.with_columns(\n",
    "        (\n",
    "            (pl.when(pl.col(\"t_event_pred\") < threshold)).then(pl.col(\"pred\")).otherwise(pl.col(\"pred\") + adjustment)\n",
    "        ).alias(\"pred\")\n",
    "    )\n",
    "\n",
    "    # Assuming Metric() is a function that calculates your score\n",
    "    # You might need to replace this with your actual metric calculation\n",
    "    score = Metric()(input_df=adjusted_df)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Grid search for optimal parameters\n",
    "def grid_search(thresholds, adjustments):\n",
    "    \"\"\"\n",
    "    Perform grid search over thresholds and adjustments\n",
    "\n",
    "    Args:\n",
    "        thresholds: List of threshold values to try\n",
    "        adjustments: List of adjustment values to try\n",
    "\n",
    "    Returns:\n",
    "        best_params: Dictionary with best threshold, adjustment, and score\n",
    "    \"\"\"\n",
    "    best_score = float(\"-inf\")  # Initialize with negative infinity\n",
    "    best_params = {\"threshold\": None, \"adjustment\": None, \"score\": None}\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Try all combinations of parameters\n",
    "    for threshold, adjustment in tqdm(product(thresholds, adjustments), total=len(thresholds) * len(adjustments)):\n",
    "        try:\n",
    "            score = calculate_score(threshold, adjustment)\n",
    "            results.append({\"threshold\": threshold, \"adjustment\": adjustment, \"score\": score})\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {\"threshold\": threshold, \"adjustment\": adjustment, \"score\": score}\n",
    "                print(f\"New best: threshold={threshold}, adjustment={adjustment}, score={score}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with threshold={threshold}, adjustment={adjustment}: {e}\")\n",
    "\n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pl.DataFrame(results)\n",
    "\n",
    "    return best_params, results_df\n",
    "\n",
    "\n",
    "# Define parameter ranges to search\n",
    "thresholds = np.linspace(0.5, 1, 50)  # Try values from 0.5 to 0.9\n",
    "adjustments = np.linspace(0.05, 0.5, 50)  # Try values from 0.05 to 0.2\n",
    "\n",
    "best_params, results_df = grid_search(thresholds, adjustments)\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "print(f\"Threshold: {best_params['threshold']}\")\n",
    "print(f\"Adjustment: {best_params['adjustment']}\")\n",
    "print(f\"Score: {best_params['score']}\")\n",
    "\n",
    "adjusted_df = va_result_agg_df.with_columns(\n",
    "    (\n",
    "        (pl.when(pl.col(\"t_event_pred\") < best_params[\"threshold\"]))\n",
    "        .then(pl.col(\"pred\"))\n",
    "        .otherwise(pl.col(\"pred\") + best_params[\"adjustment\"])\n",
    "    ).alias(\"pred\")\n",
    ")\n",
    "\n",
    "final_score = Metric()(input_df=adjusted_df)\n",
    "print(f\"Final score with best parameters: {final_score}\")\n",
    "\n",
    "# save best params\n",
    "with open(f\"{config.OUTPUT_DIR}/pp_best_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug test\n",
    "test_features_df = fe(config=config, train_test_df=train_test_df, output_dataset=\"TEST\")\n",
    "te_result_df = pl.DataFrame()\n",
    "for seed in config.SEEDS:\n",
    "    name = f\"cat_{seed}\"\n",
    "\n",
    "    _te_result_df = single_inference_fn(\n",
    "        model=CatBoostRegressorWrapper(\n",
    "            name=name,\n",
    "            feature_names=feature_names,\n",
    "            cat_features=cat_features,\n",
    "        ),\n",
    "        features_df=test_features_df,\n",
    "        feature_names=feature_names,\n",
    "        model_dir=config.ARTIFACT_EXP_DIR(),\n",
    "        inference_folds=list(range(config.N_SPLITS)),\n",
    "        out_dir=config.OUTPUT_DIR,\n",
    "    )\n",
    "    te_result_df = pl.concat([te_result_df, _te_result_df], how=\"diagonal_relaxed\")\n",
    "\n",
    "te_result_agg_df = (\n",
    "    te_result_df.group_by(config.ID_COL)\n",
    "    .agg(pl.col(\"pred\").mean())\n",
    "    .sort(\"ID\")\n",
    "    .join(train_test_df.select(config.META_COLS), on=config.ID_COL, how=\"left\")\n",
    ")\n",
    "\n",
    "all_te_result_df = pl.DataFrame()\n",
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    _te_result_df = single_inference_fn(\n",
    "        model=LightGBMWapper(name=f\"lgb1_{seed}\"),\n",
    "        features_df=test_features_df,\n",
    "        feature_names=feature_names,\n",
    "        model_dir=config.ARTIFACT_EXP_DIR(),\n",
    "        inference_folds=list(range(config.N_SPLITS)),\n",
    "        out_dir=config.OUTPUT_DIR,\n",
    "    )\n",
    "    all_te_result_df = pl.concat(\n",
    "        [\n",
    "            all_te_result_df,\n",
    "            _te_result_df.select(\n",
    "                [\n",
    "                    pl.col(config.ID_COL),\n",
    "                    pl.col(\"pred\"),\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        how=\"diagonal_relaxed\",\n",
    "    )\n",
    "    _te_result_df = single_inference_fn(\n",
    "        model=CatBoostClassifierWrapper(\n",
    "            name=f\"cat1_{seed}\",\n",
    "            feature_names=feature_names,\n",
    "            cat_features=cat_features,\n",
    "        ),\n",
    "        features_df=test_features_df,\n",
    "        feature_names=feature_names,\n",
    "        model_dir=config.ARTIFACT_EXP_DIR(),\n",
    "        inference_folds=list(range(config.N_SPLITS)),\n",
    "        out_dir=config.OUTPUT_DIR,\n",
    "    )\n",
    "    all_te_result_df = pl.concat(\n",
    "        [\n",
    "            all_te_result_df,\n",
    "            _te_result_df.select(\n",
    "                [\n",
    "                    pl.col(config.ID_COL),\n",
    "                    pl.col(\"pred\"),\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        how=\"diagonal_relaxed\",\n",
    "    )\n",
    "\n",
    "agg_te_result_df = all_te_result_df.group_by(config.ID_COL).agg(pl.col(\"pred\").mean().alias(\"t_event_pred\")).sort(\"ID\")\n",
    "te_result_agg_df = te_result_agg_df.select(pl.exclude(\"t_event_pred\")).join(\n",
    "    agg_te_result_df,\n",
    "    on=config.ID_COL,\n",
    "    how=\"left\",\n",
    ")\n",
    "print(te_result_agg_df)\n",
    "\n",
    "# post process\n",
    "te_result_agg_df = te_result_agg_df.with_columns(\n",
    "    (\n",
    "        (pl.when(pl.col(\"t_event_pred\") < best_params[\"threshold\"]))\n",
    "        .then(pl.col(\"pred\"))\n",
    "        .otherwise(pl.col(\"pred\") + best_params[\"adjustment\"])\n",
    "    ).alias(\"pred\")\n",
    ")\n",
    "print(te_result_agg_df)\n",
    "\n",
    "print(te_result_agg_df[\"pred\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
