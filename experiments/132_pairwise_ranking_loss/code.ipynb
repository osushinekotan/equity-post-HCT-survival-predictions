{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import config\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from preprocess import fe, load_data, preprocess\n",
    "from pytorch_tabular.models.common.layers import ODST\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from torch import nn\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from src.customs.fold import add_kfold\n",
    "from src.customs.metrics import Metric, metric\n",
    "from src.trainer.tabular.simple import single_inference_fn, single_train_fn\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = load_data(config=config, valid_ratio=config.VALID_RATIO)\n",
    "target_df = pl.read_csv(\"./data/extr_output/101/1/101.csv\").with_columns(\n",
    "    pl.col(config.SURVIVAL_TIME_COL).log().alias(\"t_log_efs_time\"),\n",
    ")\n",
    "target_df = target_df.with_columns(\n",
    "    pl.Series(\"t_efs_time_scaled\", minmax_scale(target_df[config.SURVIVAL_TIME_COL], feature_range=(0, 1)))\n",
    ")\n",
    "\n",
    "\n",
    "target_cols = [x for x in target_df.columns if x.startswith(\"t_\")]\n",
    "train_test_df = train_test_df.join(\n",
    "    target_df.select(\n",
    "        [\n",
    "            config.ID_COL,\n",
    "            *target_cols,\n",
    "        ],\n",
    "    ),\n",
    "    on=config.ID_COL,\n",
    "    how=\"left\",\n",
    ")\n",
    "config.META_COLS = set(config.META_COLS) | set(target_cols)\n",
    "\n",
    "features_df = fe(config=config, train_test_df=train_test_df)\n",
    "features_df = preprocess(config=config, features_df=features_df)\n",
    "feature_names = sorted([x for x in features_df.columns if x.startswith(config.FEATURE_PREFIX)])\n",
    "cat_features = [x for x in feature_names if x.startswith(f\"{config.FEATURE_PREFIX}c_\")]\n",
    "cont_features = [x for x in feature_names if x.startswith(f\"{config.FEATURE_PREFIX}n_\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module for the categorical dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, projection_dim: int, categorical_cardinality: list[int], embedding_dim: int):\n",
    "        \"\"\"\n",
    "        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n",
    "        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n",
    "        embedding_dim: The size of the embedding space for each categorical feature.\n",
    "        self.embeddings: list of embedding layers for each categorical feature.\n",
    "        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(cardinality, embedding_dim) for cardinality in categorical_cardinality]\n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(projection_dim, projection_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat):\n",
    "        \"\"\"\n",
    "        Apply the projection on concatened embeddings that contains all categorical features.\n",
    "        \"\"\"\n",
    "        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        x_cat = torch.cat(x_cat, dim=1)\n",
    "        return self.projection(x_cat)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Train a model on both categorical embeddings and numerical data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        continuous_dim: int,\n",
    "        categorical_cardinality: list[int],\n",
    "        embedding_dim: int,\n",
    "        projection_dim: int,\n",
    "        hidden_dim: int,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = CatEmbedding(projection_dim, categorical_cardinality, embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            ODST(projection_dim + continuous_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.main_out = nn.Linear(hidden_dim, 1)\n",
    "        self.aux_out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_cat, x_cont, **kwargs):\n",
    "        \"\"\"\n",
    "        Create embedding layers for categorical data, concatenate with continous variables.\n",
    "        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x_cat)\n",
    "        x = torch.cat([x, x_cont], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mlp(x)\n",
    "        return self.main_out(x), self.aux_out(x)\n",
    "\n",
    "\n",
    "@functools.lru_cache\n",
    "def combinations(N):\n",
    "    \"\"\"\n",
    "    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1,\n",
    "    and caches the result using functools.lru_cache for optimization\n",
    "    \"\"\"\n",
    "    ind = torch.arange(N)\n",
    "    comb = torch.combinations(ind, r=2)\n",
    "    return comb.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Main Model creation and losses definition to fully train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,  # partial\n",
    "        scheduler: torch.optim.lr_scheduler,  # partial\n",
    "        aux_weight: float = 0.1,\n",
    "        margin: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)  # to use self.hparams\n",
    "\n",
    "        # Creates an instance of the NN model defined above\n",
    "        self.model = net\n",
    "        self.results = []\n",
    "\n",
    "        # for averaging loss across batches\n",
    "        self.train_main_loss = MeanMetric()\n",
    "        self.train_loss = MeanMetric()\n",
    "\n",
    "        self.val_main_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, x_aux = self.model(x_cat=batch[\"x_cat\"], x_cont=batch[\"x_cont\"])\n",
    "        return x.squeeze(1), x_aux.squeeze(1)\n",
    "\n",
    "    def model_step(self, batch):\n",
    "        y_pred, aux_pred = self.forward(batch)\n",
    "        main_loss = self.calc_main_loss(time=batch[\"main_target\"], event=batch[\"event\"], y_pred=y_pred)\n",
    "        aux_loss = nn.functional.mse_loss(aux_pred, batch[\"aux_target\"], reduction=\"mean\")\n",
    "        loss = main_loss + (aux_loss * self.hparams.aux_weight)\n",
    "        # loss = aux_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"main_loss\": main_loss,\n",
    "            \"aux_loss\": aux_loss,\n",
    "            \"y_pred\": y_pred,\n",
    "            \"aux_pred\": aux_pred,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Perform a single training step on a batch of data from the training set.\n",
    "\n",
    "        :param batch: A batch of data (a tuple) containing the input tensor of images and target\n",
    "            labels.\n",
    "        :param batch_idx: The index of the current batch.\n",
    "        :return: A tensor of losses between model predictions and targets.\n",
    "        \"\"\"\n",
    "        result = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.train_main_loss(result[\"main_loss\"])\n",
    "        self.train_loss(result[\"loss\"])\n",
    "        self.log(\"train/main_loss\", self.train_main_loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "\n",
    "        return result[\"loss\"]\n",
    "\n",
    "    def calc_main_loss(self, time, event, y_pred):\n",
    "        N = time.shape[0]\n",
    "        pairs = combinations(N)\n",
    "\n",
    "        # 比較可能な候補ペアのみを取得\n",
    "        pairs = pairs[(event[pairs[:, 0]] == 1) | (event[pairs[:, 1]] == 1)]\n",
    "\n",
    "        left_index, right_index = pairs[:, 0], pairs[:, 1]\n",
    "        time_left, time_right = time[left_index], time[right_index]\n",
    "        event_left, event_right = event[left_index], event[right_index]\n",
    "        y_pred_left, y_pred_right = y_pred[left_index], y_pred[right_index]\n",
    "\n",
    "        # calculate the loss\n",
    "        y = 2 * (time_left - time_right).int() - 1\n",
    "        diff = y_pred_right - y_pred_left\n",
    "        loss = nn.functional.relu(-y * (diff) + self.hparams.margin)\n",
    "\n",
    "        # loss 計算対象 mask\n",
    "        mask = self._get_mask(\n",
    "            time_left=time_left,\n",
    "            time_right=time_right,\n",
    "            event_left=event_left,\n",
    "            event_right=event_right,\n",
    "        )\n",
    "        loss = (loss.double() * (mask.double())).sum() / mask.sum()  # mean across batch\n",
    "        return loss\n",
    "\n",
    "    def _get_mask(self, time_left, time_right, event_left, event_right):\n",
    "        # Case 1: left が right より生存時間が長いが、right が censored されている場合\n",
    "        left_outlived = time_left >= time_right\n",
    "        left_1_right_0 = (event_left == 1) & (event_right == 0)\n",
    "\n",
    "        # Case 2: right が left より生存時間が長いが、left が censored されている場合\n",
    "        right_outlived = time_right >= time_left\n",
    "        right_1_left_0 = (event_right == 1) & (event_left == 0)\n",
    "\n",
    "        # Combine the masks\n",
    "        mask = (left_outlived & left_1_right_0) | (right_outlived & right_1_left_0)\n",
    "        return ~mask  # Invert the mask to get the valid pairs\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        result = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.val_main_loss(result[\"main_loss\"])\n",
    "        self.val_loss(result[\"loss\"])\n",
    "        self.log(\"val/main_loss\", self.val_main_loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "\n",
    "        # store time, event, y_pred, aux_pred for c-index calculation\n",
    "        self.results.append(\n",
    "            {\n",
    "                **{k: v for k, v in result.items() if k in [\"y_pred\", \"aux_pred\"]},\n",
    "                **{k: v for k, v in batch.items() if k in [\"time\", \"event\", \"race_group\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def calc_metric(\n",
    "        self,\n",
    "        time: np.ndarray,\n",
    "        event: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        aux_pred: np.ndarray,\n",
    "        blend_pred: np.ndarray,\n",
    "        race_group: np.ndarray,\n",
    "    ):\n",
    "        score_y = metric(y_time=time, y_event=event, y_pred=y_pred, race_group=race_group)\n",
    "        score_aux = metric(y_time=time, y_event=event, y_pred=aux_pred, race_group=race_group)\n",
    "        score_blend = metric(y_time=time, y_event=event, y_pred=blend_pred, race_group=race_group)\n",
    "        return score_y, score_aux, score_blend\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        At the end of the validation epoch, it computes and logs the concordance index\n",
    "        \"\"\"\n",
    "        time = np.concatenate([r[\"time\"].cpu().numpy() for r in self.results])\n",
    "        event = np.concatenate([r[\"event\"].cpu().numpy() for r in self.results])\n",
    "        y_pred = np.concatenate([r[\"y_pred\"].cpu().numpy() for r in self.results])\n",
    "        aux_pred = np.concatenate([r[\"aux_pred\"].cpu().numpy() for r in self.results])\n",
    "        race_group = np.concatenate([r[\"race_group\"].cpu().numpy() for r in self.results])\n",
    "\n",
    "        # ensemble: rankdata -> normalize -> mean\n",
    "        blend_pred = np.sum([rankdata(y_pred), rankdata(aux_pred)], axis=0)\n",
    "\n",
    "        score_y, score_aux, score_blend = self.calc_metric(\n",
    "            time=time,\n",
    "            event=event,\n",
    "            y_pred=y_pred,\n",
    "            aux_pred=aux_pred,\n",
    "            blend_pred=blend_pred,\n",
    "            race_group=race_group,\n",
    "        )\n",
    "        self.log(\"val/score\", score_y, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/aux_score\", score_aux, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/blend_score\", score_blend, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        self.results = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())\n",
    "        if self.hparams.scheduler is not None:\n",
    "            scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val/loss\",\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        return {\"optimizer\": optimizer}\n",
    "\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        cat_feature_cols: list[str],\n",
    "        cont_feature_cols: list[str],\n",
    "        main_target_col: str,\n",
    "        aux_target_col: str,\n",
    "        time_col: str,\n",
    "        event_col: str,\n",
    "        race_group_col: str,\n",
    "    ):\n",
    "        self.cat_features = df.select(cat_feature_cols).to_numpy()\n",
    "        self.cont_features = df.select(cont_feature_cols).to_numpy()\n",
    "        self.time = df[time_col].to_numpy()\n",
    "        self.event = df[event_col].to_numpy()\n",
    "        self.main_target = df[main_target_col].to_numpy()\n",
    "        self.aux_target = df[aux_target_col].to_numpy()\n",
    "        self.race_group = df[race_group_col].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"x_cat\": torch.tensor(self.cat_features[idx], dtype=torch.long),\n",
    "            \"x_cont\": torch.tensor(self.cont_features[idx], dtype=torch.float),\n",
    "            \"time\": torch.tensor(self.time[idx], dtype=torch.float),\n",
    "            \"event\": torch.tensor(self.event[idx], dtype=torch.long),\n",
    "            \"main_target\": torch.tensor(self.main_target[idx], dtype=torch.float),\n",
    "            \"aux_target\": torch.tensor(self.aux_target[idx], dtype=torch.float),\n",
    "            \"race_group\": torch.tensor(self.race_group[idx]),\n",
    "        }\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        cat_feature_cols: list[str],\n",
    "        cont_feature_cols: list[str],\n",
    "        race_group_col: str,\n",
    "    ):\n",
    "        self.cat_features = df.select(cat_feature_cols).to_numpy()\n",
    "        self.cont_features = df.select(cont_feature_cols).to_numpy()\n",
    "        self.race_group = df[race_group_col].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"x_cat\": torch.tensor(self.cat_features[idx], dtype=torch.long),\n",
    "            \"x_cont\": torch.tensor(self.cont_features[idx], dtype=torch.float),\n",
    "            \"race_group\": torch.tensor(self.race_group[idx]),\n",
    "        }\n",
    "\n",
    "\n",
    "def inference_fn(dataloader, model, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)[0]  # multioutput\n",
    "            predictions.append(outputs.cpu())\n",
    "\n",
    "    predictions = torch.cat(predictions, dim=0).numpy().reshape(-1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "MAX_EPOCHS = 32\n",
    "\n",
    "va_result_df, va_scores = pl.DataFrame(), {}\n",
    "\n",
    "categorical_cardinality = [features_df[f].n_unique() + 1 for f in cat_features]\n",
    "\n",
    "for seed in config.SEEDS:\n",
    "    L.seed_everything(seed)\n",
    "    name = f\"pair_{seed}\"\n",
    "    i_features_df = add_kfold(\n",
    "        features_df,\n",
    "        n_splits=config.N_SPLITS,\n",
    "        random_state=seed,\n",
    "        fold_col=config.FOLD_COL,\n",
    "    )\n",
    "\n",
    "    _va_result_df, _scores = pl.DataFrame(), {}\n",
    "    for i_fold in i_features_df[config.FOLD_COL].unique().sort():\n",
    "        fold_name = f\"fold_{i_fold:02}\"\n",
    "        print(f\"🚀 Start training: {name} - {fold_name}\")\n",
    "        tr_dataloader = torch.utils.data.DataLoader(\n",
    "            TrainDataset(\n",
    "                df=i_features_df.filter(pl.col(config.FOLD_COL) != i_fold),\n",
    "                cat_feature_cols=cat_features,\n",
    "                cont_feature_cols=cont_features,\n",
    "                main_target_col=\"t_efs_time_scaled\",\n",
    "                aux_target_col=\"t_kmf\",\n",
    "                time_col=config.SURVIVAL_TIME_COL,\n",
    "                event_col=config.EVENT_COL,\n",
    "                race_group_col=\"f_c_race_group\",\n",
    "            ),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        va_dataloader = torch.utils.data.DataLoader(\n",
    "            TrainDataset(\n",
    "                df=i_features_df.filter(pl.col(config.FOLD_COL) == i_fold),\n",
    "                cat_feature_cols=cat_features,\n",
    "                cont_feature_cols=cont_features,\n",
    "                main_target_col=\"t_efs_time_scaled\",\n",
    "                aux_target_col=\"t_kmf\",\n",
    "                time_col=config.SURVIVAL_TIME_COL,\n",
    "                event_col=config.EVENT_COL,\n",
    "                race_group_col=\"f_c_race_group\",\n",
    "            ),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        # Create the model\n",
    "        net = Net(\n",
    "            continuous_dim=len(cont_features),\n",
    "            categorical_cardinality=categorical_cardinality,  # full cardinality\n",
    "            embedding_dim=16,\n",
    "            projection_dim=24,\n",
    "            hidden_dim=64,\n",
    "            dropout=0,\n",
    "        )\n",
    "        optimizer = functools.partial(torch.optim.AdamW, lr=0.001, weight_decay=0)\n",
    "        scheduler = functools.partial(torch.optim.lr_scheduler.ReduceLROnPlateau, mode=\"min\", factor=0.1, patience=10)\n",
    "\n",
    "        lit_module = LitModule(\n",
    "            net=net,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            aux_weight=1,\n",
    "            margin=0.2,\n",
    "        )\n",
    "\n",
    "        output_dir = config.OUTPUT_DIR / name / fold_name\n",
    "        trainer = L.Trainer(\n",
    "            # accelerator=\"cpu\",\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            callbacks=[\n",
    "                L.pytorch.callbacks.ModelCheckpoint(\n",
    "                    monitor=\"val/score\",\n",
    "                    dirpath=output_dir.as_posix(),\n",
    "                    save_top_k=1,\n",
    "                    mode=\"max\",\n",
    "                    enable_version_counter=False,\n",
    "                    auto_insert_metric_name=False,\n",
    "                    filename=\"model\",\n",
    "                    save_weights_only=True,\n",
    "                ),\n",
    "                L.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "                L.pytorch.callbacks.TQDMProgressBar(),\n",
    "                # L.pytorch.callbacks.EarlyStopping(monitor=\"val/score\", patience=10, mode=\"max\"),\n",
    "            ],\n",
    "            logger=L.pytorch.loggers.CSVLogger(output_dir.as_posix(), name=\"logs\", version=\"latest\"),\n",
    "        )\n",
    "\n",
    "        # train\n",
    "        trainer.fit(lit_module, tr_dataloader, va_dataloader)\n",
    "\n",
    "        # save only best weight for inference\n",
    "        best_model = LitModule.load_from_checkpoint(output_dir / \"model.ckpt\").model\n",
    "        torch.save(best_model.state_dict(), output_dir / \"model.ckpt\")\n",
    "\n",
    "        # validation\n",
    "        val_preds = inference_fn(dataloader=va_dataloader, model=best_model, device=\"cuda\")\n",
    "        i_va_result_df = (\n",
    "            i_features_df.filter(pl.col(config.FOLD_COL) == i_fold)\n",
    "            .select(config.META_COLS)\n",
    "            .with_columns(pl.Series(\"pred\", val_preds))\n",
    "        )\n",
    "        i_score = Metric()(input_df=i_va_result_df)\n",
    "        print(f\"✅ {name} - {fold_name} - score: {i_score}\")\n",
    "        _scores[fold_name] = i_score\n",
    "        _va_result_df = pl.concat([_va_result_df, i_va_result_df], how=\"diagonal_relaxed\")\n",
    "\n",
    "    # save scores\n",
    "    with open(config.OUTPUT_DIR / name / \"va_scores.json\", \"w\") as f:\n",
    "        json.dump(_scores, f, indent=4)\n",
    "\n",
    "    va_scores[name] = _scores\n",
    "    va_result_df = pl.concat([va_result_df, _va_result_df], how=\"diagonal_relaxed\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# final score\n",
    "# ------------------------------\n",
    "va_result_agg_df = (\n",
    "    va_result_df.group_by(config.ID_COL)\n",
    "    .agg(pl.col(\"pred\").mean())\n",
    "    .sort(\"ID\")\n",
    "    .join(train_test_df.select(config.META_COLS), on=config.ID_COL, how=\"left\")\n",
    ")\n",
    "final_score = Metric()(input_df=va_result_agg_df)\n",
    "logger.info(f\"✅ final score: {final_score}\")\n",
    "va_scores[\"final\"] = final_score\n",
    "\n",
    "# save\n",
    "va_result_agg_df.write_csv(f\"{config.OUTPUT_DIR}/va_result.csv\")\n",
    "with open(f\"{config.OUTPUT_DIR}/va_scores.json\", \"w\") as f:\n",
    "    json.dump(va_scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
